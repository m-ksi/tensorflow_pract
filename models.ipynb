{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebcc42a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "print(tf.__version__)\n",
    "tf.random.set_seed(42) # The Ultimate Question of Life, the Universe, and Everything\n",
    "\n",
    "true_weights = tf.constant(list(range(5)), dtype=tf.float32)[:, tf.newaxis]\n",
    "x = tf.constant(tf.random.uniform((32, 5)), dtype=tf.float32)\n",
    "y = tf.constant(x @ true_weights, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0290e",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8df400",
   "metadata": {},
   "source": [
    "Model is a set of parameters and the computation methods using these parameters. It makes sense to create a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c5d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, num_parameters):\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_parameters, 1)), dtype=tf.float32)\n",
    "        \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        return tf.linalg.matmul(x, self._weights)\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self._weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66946223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE at iteration    0 is 15.1177\n",
      "MSE at iteration  200 is 0.0278\n",
      "MSE at iteration  400 is 0.0012\n",
      "MSE at iteration  600 is 0.0001\n",
      "MSE at iteration  800 is 0.0000\n",
      "MSE at iteration 1000 is 0.0000\n",
      "<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-1.9200621e-03],\n",
      "       [ 1.0043813e+00],\n",
      "       [ 2.0000753e+00],\n",
      "       [ 3.0021193e+00],\n",
      "       [ 3.9955368e+00]], dtype=float32)>\n",
      "time took: 3.477302312850952 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(5)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(y-y_hat))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    model.variables.assign_add(tf.constant([-0.05], dtype=tf.float32) * gradients)\n",
    "    return loss\n",
    "\n",
    "t0 = time.time()\n",
    "for iteration in range(1001):\n",
    "    loss = train_step()\n",
    "    if not (iteration % 200):\n",
    "        print('MSE at iteration {:4d} is {:5.4f}'.format(iteration, loss))\n",
    "pprint(model.variables)\n",
    "print(f'time took: {time.time() - t0} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee5740",
   "metadata": {},
   "source": [
    "works fine, but it can be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc8c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(tf.keras.Model):\n",
    "    def __init__(self, num_parameters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_parameters, 1)), dtype=tf.float32)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x): #keras model's __call__ is a wrapper over this call lol\n",
    "        return tf.linalg.matmul(x, self._weights)\n",
    "    # .variables already here, returns the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5e2e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE at iteration    0 is 18.7201\n",
      "MSE at iteration  200 is 0.0325\n",
      "MSE at iteration  400 is 0.0017\n",
      "MSE at iteration  600 is 0.0002\n",
      "MSE at iteration  800 is 0.0000\n",
      "MSE at iteration 1000 is 0.0000\n",
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-3.1363715e-03],\n",
      "       [ 1.0065705e+00],\n",
      "       [ 2.0000944e+00],\n",
      "       [ 3.0032609e+00],\n",
      "       [ 3.9934683e+00]], dtype=float32)>]\n",
      "time took: 0.2891373634338379 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(5)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    \n",
    "    for g, v in zip(gradients, model.variables):\n",
    "        v.assign_add(tf.constant([-0.05], dtype=tf.float32) * g)\n",
    "    return loss\n",
    "\n",
    "t0 = time.time()\n",
    "for iteration in range(1001):\n",
    "    loss = train_step()\n",
    "    if not (iteration % 200):\n",
    "        print('MSE at iteration {:4d} is {:5.4f}'.format(iteration, loss))\n",
    "        \n",
    "pprint(model.variables)\n",
    "print(f'time took: {time.time() - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b4cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"linear_regression\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[4.435382834344637e-06, 0.0016956925392150879]\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "model.compile(loss='mse', metrics=['mae'])\n",
    "print(model.evaluate(x, y, verbose=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e52dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.022109985351562,\n",
      " 0.03321070224046707,\n",
      " 0.0017082320991903543,\n",
      " 0.000158100068802014,\n",
      " 2.3927499569253996e-05,\n",
      " 4.314993930165656e-06]\n",
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-3.0572712e-03],\n",
      "       [ 1.0064490e+00],\n",
      "       [ 2.0001047e+00],\n",
      "       [ 3.0031922e+00],\n",
      "       [ 3.9935679e+00]], dtype=float32)>]\n",
      "time took: 1.6570169925689697 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(5)\n",
    "model.compile(optimizer='SGD', loss='mse')\n",
    "model.optimizer.lr.assign(.05)\n",
    "\n",
    "t0 = time.time()\n",
    "history = model.fit(x, y, epochs = 1001, verbose=0)\n",
    "pprint(history.history['loss'][::200])\n",
    "pprint(model.variables)\n",
    "print(f'time took: {time.time() - t0} seconds')\n",
    "# works longer due to convenience of .fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d9704",
   "metadata": {},
   "source": [
    "### adding useless bias to check if model handles it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be1ba845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[0.803156  ],\n",
      "       [0.49777734],\n",
      "       [0.37054038],\n",
      "       [0.9118674 ],\n",
      "       [0.637642  ]], dtype=float32)>,\n",
      " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([100.], dtype=float32)>]\n",
      "time took: 0.19468259811401367 seconds\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionV2(tf.keras.Model):\n",
    "    def __init__(self, num_parameters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_parameters, 1)), dtype=tf.float32)\n",
    "        self._bias = tf.Variable([100], dtype=tf.float32)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.linalg.matmul(x, self._weights) + self._bias\n",
    "    \n",
    "model = LinearRegressionV2(5)\n",
    "\n",
    "t0 = time.time()\n",
    "for iteration in range(1001):\n",
    "    loss = train_step()\n",
    "        \n",
    "pprint(model.variables)\n",
    "print(f'time took: {time.time() - t0} seconds')\n",
    "# doesn't seem to see bias at all! tf is using same graph as before for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b2efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[5.6935853e-04],\n",
      "       [1.0001738e+00],\n",
      "       [1.9999688e+00],\n",
      "       [2.9999273e+00],\n",
      "       [3.9994380e+00]], dtype=float32)>]\n",
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-2.6784972e-03],\n",
      "       [ 9.9403036e-01],\n",
      "       [ 1.9959843e+00],\n",
      "       [ 2.9956350e+00],\n",
      "       [ 3.9983540e+00]], dtype=float32)>,\n",
      " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.00965639], dtype=float32)>]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(y-y_hat))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "        \n",
    "    for g, v in zip(gradients, model.variables):\n",
    "        v.assign_add(tf.constant([-0.05], dtype=tf.float32) * g)\n",
    "    return loss\n",
    "\n",
    "model = LinearRegression(5)\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "pprint(model.variables)\n",
    "\n",
    "model = LinearRegressionV2(5)\n",
    "for iteration in range(5001):\n",
    "    loss = train_step(model)\n",
    "pprint(model.variables)\n",
    "\n",
    "print(train_step._get_tracing_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864dae0",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8053e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_inputs, num_outputs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_inputs, num_outputs)), dtype=tf.float32)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.linalg.matmul(x, self._weights)\n",
    "\n",
    "class Regression(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_inputs_per_layer, num_outputs_per_layer, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = [Linear(num_inputs, num_outputs)\n",
    "                       for (num_inputs, num_outputs) in zip(num_inputs_per_layer, num_outputs_per_layer)]\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13bdae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is: 1.3709068e-06\n"
     ]
    }
   ],
   "source": [
    "model = Regression([5, 3], [3, 1])\n",
    "\n",
    "for it in range(1001):\n",
    "    loss = train_step(model)\n",
    "\n",
    "print('MAE is:', tf.reduce_mean(tf.abs(y-model(x))).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7aa60",
   "metadata": {},
   "source": [
    "One problem with this linear layer is that it needs the complete sizing information and\n",
    "allocates resources for all the variables upfront. Ideally, we want it to be a bit lazy, it\n",
    "should calculate variable sizes and occupy resources only when needed. To archive this,\n",
    "we implement the build method which will handle the variable initialization. The\n",
    "build method can be explicitly called, or it will be invoked automatically the first time\n",
    "there is data flow to it. With this, the constructor now only stores the hyperparameters for\n",
    "the layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c3bfda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "MAE is:  3.3527613e-07\n",
      "[<tf.Variable 'linear_4/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-0.6077952 ,  0.4016254 ,  0.05021823],\n",
      "       [-0.66946924, -0.04704069,  0.51261395],\n",
      "       [-0.7182107 ,  0.36316237,  0.95652574],\n",
      "       [ 0.34688628, -0.59620196,  1.321127  ],\n",
      "       [ 0.88979864,  0.49720666,  1.6945884 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_5/Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.21429297],\n",
      "       [0.04486551],\n",
      "       [2.2347693 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self._weights = self.add_weight(shape=(input_shape[-1], self.units))\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        output = tf.linalg.matmul(x, self._weights)\n",
    "        return output\n",
    "    \n",
    "class Regression(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = [Linear(unit) for unit in units]\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "model = Regression([3, 1])\n",
    "pprint(model.variables) # should be empty\n",
    "\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "    \n",
    "print('MAE is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204dccc9",
   "metadata": {},
   "source": [
    "honestly it's just easier to use Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67080322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "MAE is:  7.376075e-07\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.5343044 ,  0.7596104 ,  0.04952784],\n",
      "       [-0.44352436, -1.0123183 , -0.7924589 ],\n",
      "       [ 0.40134627, -0.81265366, -0.4920736 ],\n",
      "       [ 0.83151644, -0.9049989 , -0.69753546],\n",
      "       [ 1.8104649 , -0.38345483, -0.5909047 ]], dtype=float32)>,\n",
      " <tf.Variable 'dense_1/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[ 1.7215422],\n",
      "       [-1.1626605],\n",
      "       [-0.740184 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class Regression(tf.keras.Model):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = [tf.keras.layers.Dense(unit, use_bias=False) for unit in units]\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "model = Regression([3, 1])\n",
    "pprint(model.variables) # should be empty\n",
    "\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "    \n",
    "print('MAE is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf77ba3",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d514a12",
   "metadata": {},
   "source": [
    "our model is just a linear transformation\n",
    "\n",
    "let's add activations as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47b4c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is:  1.0803342e-06\n",
      "[<tf.Variable 'linear_8/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-0.23759118, -0.15370896,  0.50241596],\n",
      "       [ 0.9812125 , -0.32753882,  0.47469217],\n",
      "       [ 0.67018586,  0.53977454,  0.3383523 ],\n",
      "       [ 0.73897237,  1.0795476 ,  0.34267387],\n",
      "       [ 0.76488936,  1.2309433 ,  1.0314057 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_9/Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[1.0873303],\n",
      "       [1.705775 ],\n",
      "       [1.0360578]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class ReLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.maximum(tf.constant(0, x.dtype), x)\n",
    "\n",
    "\n",
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, units, last_linear=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        layers = []\n",
    "        n = len(units)\n",
    "        for i, unit in enumerate(units):\n",
    "            layers.append(Linear(unit))\n",
    "            if i < n - 1 or not last_linear:\n",
    "                layers.append(ReLU())\n",
    "        self._layers = layers\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork([3, 1])\n",
    "\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "\n",
    "print('MAE is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "\n",
    "pprint(model.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3e080",
   "metadata": {},
   "source": [
    "non-linearity made it harder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cb400",
   "metadata": {},
   "source": [
    "next idea is to fuse linear and non-linear together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25e052a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is:  5.505979e-06\n",
      "[<tf.Variable 'linear_12/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.5395865 , -0.6896926 , -0.9564117 ],\n",
      "       [ 0.10679358,  0.7153631 ,  0.81789774],\n",
      "       [ 0.02986215, -0.46865082,  1.3411657 ],\n",
      "       [ 1.2822582 , -0.5852419 ,  0.31231666],\n",
      "       [ 1.8721235 ,  0.6778295 ,  0.6126671 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_12/Variable:0' shape=(3,) dtype=float32, numpy=array([-0.14929764,  1.107722  ,  0.25128555], dtype=float32)>,\n",
      " <tf.Variable 'linear_13/Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[ 1.8443246 ],\n",
      "       [-0.38262856],\n",
      "       [ 1.3164601 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_13/Variable:0' shape=(1,) dtype=float32, numpy=array([0.36842757], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, activation='linear', **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activation\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self._weights = self.add_weight(shape=(input_shape[-1], self.units))\n",
    "        if self.use_bias:\n",
    "            self._bias = self.add_weight(shape=(self.units), initializer='ones')\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        output = tf.linalg.matmul(x, self._weights)\n",
    "        if self.use_bias:\n",
    "            output += self._bias\n",
    "        if self.activation == 'relu':\n",
    "            outpus = tf.maximum(tf.constant(0, x.dtype), output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, units, use_bias=True, last_linear=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        layers = [Linear(unit, use_bias, 'relu') for unit in units[:-1]]\n",
    "        layers.append(Linear(units[-1], use_bias, 'linear' if last_linear else 'relu'))\n",
    "        self._layers = layers\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork([3, 1])\n",
    "\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "\n",
    "print('MAE is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041d317",
   "metadata": {},
   "source": [
    "### FC Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63c031da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(tf.keras.Model):\n",
    "    def __init__(self, layers, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = layers\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class MLP(tf.keras.Model): # multi-layer perceptron\n",
    "    def __init__(self, num_hidden_units, num_targets, hidden_activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if type(num_hidden_units) is int: num_hidden_units = [num_hidden_units]\n",
    "        self.feature_extractor = Sequential([tf.keras.layers.Dense(unit, activation=hidden_activation) for unit in num_hidden_units])\n",
    "        self.last_linear = tf.keras.layers.Dense(num_targets, activation='linear')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        outputs = self.last_linear(features)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d6cab",
   "metadata": {},
   "source": [
    "I'll test in on boston housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
    "y_train, y_test = map(lambda x: np.expand_dims(x, -1), (y_train, y_test))\n",
    "X_train, y_train, X_test, y_test = map(lambda x: tf.cast(x, tf.float32), (X_train, y_train, X_test, y_test))\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_mean(tf.square(y - model(x)))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.variables)\n",
    "        for g, v in zip(gradients, model.variables):\n",
    "            v.assign_add(tf.constant([-0.01], dtype=tf.float32) * g)\n",
    "        return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y):\n",
    "    return tf.reduce_mean(tf.square(y - model(x)))\n",
    "\n",
    "def train(model, n_epochs=1000, his_freq=10):\n",
    "    history = []\n",
    "    for iteration in range(1, n_epochs + 1):\n",
    "        tr_loss = train_step(model, X_train, y_train)\n",
    "        te_loss = test_step(model, X_test, y_test)\n",
    "        if not ineration % his_freq:\n",
    "            history.append({\n",
    "                'iteration': iteration,\n",
    "                'training_loss': tr_loss.numpy(),\n",
    "                'testing_loss': te_loss.numpy()\n",
    "            })\n",
    "    return model, pd.DataFrame(history)\n",
    "\n",
    "mlp, mlp_history = train(MLP(4, 1))\n",
    "pprint(mlp_history.tail())\n",
    "ax = mlp_history.plot(x='iteration', kind='line', logy=True)\n",
    "fig = ax.get_figure()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
